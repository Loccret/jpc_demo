{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3393ed77",
   "metadata": {},
   "source": [
    "Let\u2019s build the smallest possible irregular-time dataset where a continuous-time model (Neural ODE\u2013style) should beat a plain 3-layer MLP that relies on uniform resampling.\n",
    "\n",
    "Quick intuition (why this works): with irregular timestamps, a fixed-grid MLP must first squash each example onto a uniform grid (zero-fill / interpolate). That throws away timing detail. A Neural-ODE pipeline can consume the actual event times and integrate to any query time, which is exactly where continuous-time models have been shown to help on uneven time series (e.g., ODE-RNN / Latent ODE, Neural CDE).\n",
    "\n",
    "Below is self-contained JAX code that:\n",
    "- Synthesizes a 1D latent ODE process with per-sequence decay rate and irregular, noisy observation times then visualize it.\n",
    "- Trains two models to predict the value at a future query time $t$ given the irregular observations:\n",
    "    1. Baseline MLP (3 layers) fed a uniformly resampled vector (plus mask & \u0394t channels).\n",
    "    2. Neural-ODE model that extracts a rate parameter from the irregular events with a tiny encoder, then integrates $\\frac{dy}{dt} = \\alpha_\\theta y$ using `diffrax.diffeqsolve` to $t$.\n",
    "    3. Compares their test performance. \n",
    "\n",
    "> Expectation: on this irregular dataset, the Neural-ODE variant typically achieves lower MSE because it respects true event times instead of relying on coarse resampling. This matches the literature\u2019s advantage of continuous-time models on uneven data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesizes a 1D latent ODE process with per-sequence decay rate and irregular, noisy observation times.\n",
    "# Trains two models to predict the value at a future query time t given the irregular observations.\n",
    "import math, functools, numpy as np\n",
    "import jax, jax.numpy as jnp, jax.random as jr\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import diffrax as dx\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic irregular dataset\n",
    "# ----------------------------\n",
    "# Latent true dynamics per sequence: dy/dt = -w * y, with y(0)=y0\n",
    "# We observe noisy y at irregular times in [0, T_obs], and want y(t_star).\n",
    "key = jr.PRNGKey(0)\n",
    "T_obs = 1.0\n",
    "t_star = 1.3                      # query time is beyond last obs (mild extrapolation)\n",
    "N_train, N_val = 4000, 1000\n",
    "min_events, max_events = 4, 16    # irregular # of events per sequence\n",
    "sigma_noise = 0.03\n",
    "\n",
    "def sample_sequence(key):\n",
    "    k1, k2, k3 = jr.split(key, 3)\n",
    "    # per-example decay rate and initial value\n",
    "    w = jr.lognormal(k1, sigma=0.4) + 0.2     # positive, varied\n",
    "    y0 = jr.normal(k2) * 0.5 + 1.0\n",
    "    # irregular times: sorted uniform then randomly drop some\n",
    "    n = jr.randint(k3, (), min_events, max_events+1)\n",
    "    ts = jnp.sort(jr.uniform(k3, shape=(n,), minval=0.0, maxval=T_obs))\n",
    "    y_clean = y0 * jnp.exp(-w * ts)\n",
    "    y_obs = y_clean + jr.normal(k3, shape=ts.shape) * sigma_noise\n",
    "    y_target = y0 * jnp.exp(-w * t_star)     # ground truth at query time\n",
    "    return ts, y_obs, y_target\n",
    "\n",
    "def batch_dataset(key, N):\n",
    "    keys = jr.split(key, N)\n",
    "    data = [sample_sequence(k) for k in keys]\n",
    "    # jagged to python lists\n",
    "    ts_list = [d[0] for d in data]\n",
    "    ys_list = [d[1] for d in data]\n",
    "    target = jnp.stack([d[2] for d in data])\n",
    "    return ts_list, ys_list, target\n",
    "\n",
    "key_train, key_val = jr.split(key)\n",
    "train_ts, train_ys, train_ystar = batch_dataset(key_train, N_train)\n",
    "val_ts,   val_ys,   val_ystar   = batch_dataset(key_val,   N_val)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Utilities: make uniform-grid tensors for the baseline\n",
    "# ----------------------------\n",
    "GRID = 16\n",
    "grid = jnp.linspace(0.0, T_obs, GRID)\n",
    "\n",
    "def to_uniform_grid(ts, ys):\n",
    "    # simple 0-order hold onto the nearest left grid point\n",
    "    # Also produce a mask (which grid bins had an observation) and \u0394t since last obs per bin.\n",
    "    # NOTE: this is deliberately simplistic; it's what hurts the baseline.\n",
    "    idx = jnp.searchsorted(grid, ts, side=\"right\") - 1\n",
    "    idx = jnp.clip(idx, 0, GRID-1)\n",
    "    y_grid = jnp.zeros((GRID,))\n",
    "    m_grid = jnp.zeros((GRID,))\n",
    "    y_grid = y_grid.at[idx].set(ys)\n",
    "    m_grid = m_grid.at[idx].set(1.0)\n",
    "    # time since last obs per bin\n",
    "    last_t = -jnp.inf * jnp.ones((GRID,))\n",
    "    last_t = last_t.at[idx].set(ts)\n",
    "    # fill missing with last seen time going left->right\n",
    "    last = -1e9\n",
    "    dt_grid = []\n",
    "    for g, t in enumerate(grid):\n",
    "        last = jnp.where(m_grid[g]>0, last_t[g], last)\n",
    "        dt_grid.append(jnp.maximum(0.0, t - jnp.maximum(last, 0.0)))\n",
    "    dt_grid = jnp.array(dt_grid)\n",
    "    # input features: [y_grid, mask, dt_grid, t_star - grid]  (the last lets MLP know the query offset)\n",
    "    tdiff = jnp.full((GRID,), t_star) - grid\n",
    "    x = jnp.stack([y_grid, m_grid, dt_grid, tdiff], axis=-1)   # (GRID, 4)\n",
    "    return x.reshape(-1)                                       # (GRID*4,)\n",
    "\n",
    "def pack_batch_uniform(batch_idx):\n",
    "    return train_uniform[jnp.array(batch_idx)]\n",
    "\n",
    "def pack_targets(y_star, batch_idx):\n",
    "    return y_star[jnp.array(batch_idx)]\n",
    "\n",
    "# Precompute uniform-grid tensors once so JIT stays happy\n",
    "train_uniform = jnp.stack([to_uniform_grid(t, y) for t, y in zip(train_ts, train_ys)])\n",
    "val_uniform = jnp.stack([to_uniform_grid(val_ts[i], val_ys[i]) for i in range(N_val)])\n",
    "\n",
    "def pad_irregular(ts_list, ys_list, max_len):\n",
    "    ts_padded, ys_padded, mask = [], [], []\n",
    "    for ts, ys in zip(ts_list, ys_list):\n",
    "        n = ts.shape[0]\n",
    "        pad = max_len - int(n)\n",
    "        ts_padded.append(jnp.pad(ts, (0, pad)))\n",
    "        ys_padded.append(jnp.pad(ys, (0, pad)))\n",
    "        mask.append(jnp.concatenate([jnp.ones(n, dtype=jnp.float32), jnp.zeros(pad, dtype=jnp.float32)]))\n",
    "    return jnp.stack(ts_padded), jnp.stack(ys_padded), jnp.stack(mask)\n",
    "\n",
    "train_ts_pad, train_ys_pad, train_mask = pad_irregular(train_ts, train_ys, max_events)\n",
    "val_ts_pad, val_ys_pad, val_mask = pad_irregular(val_ts, val_ys, max_events)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Baseline: 3-layer MLP on uniform grid\n",
    "# ----------------------------\n",
    "class BaselineMLP(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(self, key):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=GRID*4, out_size=1, width_size=128, depth=3,\n",
    "            activation=jax.nn.silu, key=key\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        return self.mlp(x).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Neural-ODE model:\n",
    "#    - tiny encoder predicts a scalar rate a_theta from (irregular) (t_i, y_i)\n",
    "#    - ODE: dy/dt = a_theta * y\n",
    "#    - integrate y(0) estimated from first obs (or small net) to t_star with diffrax\n",
    "# ----------------------------\n",
    "class IrregularEncoder(eqx.Module):\n",
    "    # Very small set encoder: average of per-event embeddings of [t, y, (t - prev_t)]\n",
    "    # (kept tiny on purpose; NODE's advantage stems from using true times & integration)\n",
    "    event_mlp: eqx.nn.MLP\n",
    "    head: eqx.nn.MLP\n",
    "    def __init__(self, key):\n",
    "        k1, k2 = jr.split(key)\n",
    "        self.event_mlp = eqx.nn.MLP(in_size=3, out_size=32, width_size=64, depth=2,\n",
    "                                    activation=jax.nn.tanh, key=k1)\n",
    "        self.head = eqx.nn.MLP(in_size=32, out_size=2, width_size=64, depth=2,\n",
    "                               activation=jax.nn.tanh, key=k2)  # outputs [a_theta, y0_hat]\n",
    "    def __call__(self, ts, ys, mask):\n",
    "        # ts, ys are padded to length max_events with mask indicating valid entries\n",
    "        dts = jnp.diff(ts, prepend=ts[:1])\n",
    "        feats = jnp.stack([ts, ys, dts], axis=-1)         # (n, 3)\n",
    "        emb = jax.vmap(self.event_mlp)(feats)             # (n, 32)\n",
    "        mask = mask[:, None]\n",
    "        masked = emb * mask\n",
    "        denom = jnp.maximum(mask.sum(), 1.0)\n",
    "        pooled = masked.sum(axis=0) / denom               # (32,)\n",
    "        out = self.head(pooled)                           # (2,)\n",
    "        a_theta, y0_hat = out[0], out[1]\n",
    "        return a_theta, y0_hat\n",
    "\n",
    "class NODEPredictor(eqx.Module):\n",
    "    enc: IrregularEncoder\n",
    "    def __init__(self, key):\n",
    "        self.enc = IrregularEncoder(key)\n",
    "\n",
    "    def __call__(self, ts, ys, mask):\n",
    "        # Encode irregular events to get rate and initial\n",
    "        a_theta, y0_hat = self.enc(ts, ys, mask)\n",
    "\n",
    "        # Define ODE: dy/dt = a_theta * y\n",
    "        def vf(t, y, args):\n",
    "            a = args\n",
    "            return a * y\n",
    "\n",
    "        term = dx.ODETerm(vf)\n",
    "        solver = dx.Tsit5()\n",
    "        # Integrate from t=0 to t_star, starting at y0_hat\n",
    "        sol = dx.diffeqsolve(\n",
    "            term, solver, t0=0.0, t1=t_star, dt0=1e-2,\n",
    "            y0=y0_hat, args=a_theta, saveat=dx.SaveAt(t1=True)\n",
    "        )\n",
    "        return sol.ys  # scalar\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Training loops\n",
    "# ----------------------------\n",
    "@eqx.filter_jit\n",
    "def mse(a, b): return jnp.mean((a - b) ** 2)\n",
    "\n",
    "def train_baseline():\n",
    "    key = jr.PRNGKey(1)\n",
    "    model = BaselineMLP(key)\n",
    "    opt = optax.adam(3e-3)\n",
    "    opt_state = opt.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss_fn(model, batch_idx):\n",
    "        x = pack_batch_uniform(batch_idx)      # (B, GRID*4)\n",
    "        y = pack_targets(train_ystar, batch_idx)                   # (B,)\n",
    "        preds = jax.vmap(model)(x)\n",
    "        return mse(preds, y)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(model, opt_state, batch_idx):\n",
    "        loss, grads = loss_fn(model, batch_idx)\n",
    "        updates, opt_state = opt.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "\n",
    "    BATCH = 128\n",
    "    steps = 1500\n",
    "    for i in range(steps):\n",
    "        idx = np.random.choice(N_train, size=BATCH, replace=False)\n",
    "        model, opt_state, loss = step(model, opt_state, idx)\n",
    "        if (i+1) % 200 == 0:\n",
    "            # quick val\n",
    "            val_idx = np.arange(N_val)\n",
    "            xval = val_uniform\n",
    "            yval = val_ystar\n",
    "            preds = jax.vmap(model)(xval)\n",
    "            print(f\"[Baseline] step {i+1:4d}  train_loss={loss.item():.4e}  val_mse={mse(preds, yval).item():.4e}\")\n",
    "    return model\n",
    "\n",
    "def train_node():\n",
    "    key = jr.PRNGKey(2)\n",
    "    model = NODEPredictor(key)\n",
    "    opt = optax.adam(3e-3)\n",
    "    opt_state = opt.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss_fn(model, ts_batch, ys_batch, mask_batch, targets):\n",
    "        preds = jax.vmap(lambda t, y, m: model(t, y, m))(ts_batch, ys_batch, mask_batch)\n",
    "        return mse(preds, targets)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(model, opt_state, batch_idx):\n",
    "        batch_idx = jnp.array(batch_idx)\n",
    "        ts_batch = train_ts_pad[batch_idx]\n",
    "        ys_batch = train_ys_pad[batch_idx]\n",
    "        mask_batch = train_mask[batch_idx]\n",
    "        target_batch = train_ystar[batch_idx]\n",
    "        loss, grads = loss_fn(model, ts_batch, ys_batch, mask_batch, target_batch)\n",
    "        updates, opt_state = opt.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "\n",
    "    BATCH = 128\n",
    "    steps = 1500\n",
    "    for i in range(steps):\n",
    "        idx = np.random.choice(N_train, size=BATCH, replace=False)\n",
    "        model, opt_state, loss = step(model, opt_state, idx)\n",
    "        if (i+1) % 200 == 0:\n",
    "            preds = jax.vmap(lambda t, y, m: model(t, y, m))(val_ts_pad, val_ys_pad, val_mask)\n",
    "            print(f\"[NODE]     step {i+1:4d}  train_loss={loss.item():.4e}  val_mse={mse(preds, val_ystar).item():.4e}\")\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Run both\n",
    "# ----------------------------\n",
    "baseline = train_baseline()\n",
    "node = train_node()\n",
    "\n",
    "# Final validation comparison\n",
    "xval = val_uniform\n",
    "pred_base = jax.vmap(baseline)(xval)\n",
    "pred_node = jax.vmap(lambda t, y, m: node(t, y, m))(val_ts_pad, val_ys_pad, val_mask)\n",
    "print(\"Final Val MSE -- Baseline:\", mse(pred_base, val_ystar).item())\n",
    "print(\"Final Val MSE -- NODE    :\", mse(pred_node, val_ystar).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efd626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}