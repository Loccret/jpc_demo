{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative PC on MNIST\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/thebuckleylab/jpc/blob/main/examples/generative_pc.ipynb)\n",
    "\n",
    "This notebook demonstrates how to train a neural network with predictive coding to generate MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install matplotlib==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including network architecture, learning rate, batch size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "LAYER_SIZES = [10, 300, 300, 784]\n",
    "ACT_FN = \"relu\"\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "N_INFER_ITERS = 100\n",
    "TEST_EVERY = 100\n",
    "N_TRAIN_ITERS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch and plot MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title data utils\n",
    "\n",
    "\n",
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "\n",
    "\n",
    "def plot_mnist_imgs(imgs, labels, n_imgs=10):\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(n_imgs):\n",
    "        plt.subplot(1, n_imgs, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n",
    "        plt.xlabel(jnp.argmax(labels, axis=1)[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Network\n",
    "\n",
    "For `jpc` to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like `nn.Sequential()` in [Equinox](https://github.com/patrick-kidger/equinox). For example, we can define a ReLU MLP with two hidden layers as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,10],\n",
      "      bias=f32[300],\n",
      "      in_features=10,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,300],\n",
      "      bias=f32[300],\n",
      "      in_features=300,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Linear(\n",
      "  weight=f32[784,300],\n",
      "  bias=f32[784],\n",
      "  in_features=300,\n",
      "  out_features=784,\n",
      "  use_bias=True\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(SEED)\n",
    "key, *subkeys = jax.random.split(key, 4)\n",
    "network = [\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(10, 300, key=subkeys[0]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(300, 300, key=subkeys[1]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Linear(300, 784, key=subkeys[2]),\n",
    "]\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the utility `jpc.get_fc_network` to define an MLP or fully connected network with some activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,10],\n",
      "      bias=f32[300],\n",
      "      in_features=10,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,300],\n",
      "      bias=f32[300],\n",
      "      in_features=300,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Linear(\n",
      "  weight=f32[784,300],\n",
      "  bias=f32[784],\n",
      "  in_features=300,\n",
      "  out_features=784,\n",
      "  use_bias=True\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "network = jpc.get_fc_network(key, LAYER_SIZES, act_fn=\"relu\")\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test\n",
    "\n",
    "A PC network can be trained in a single line of code with `jpc.make_pc_step()`. See the documentation for more. Similarly, we can use `jpc.test_generative_pc()` to get some test metrics including accuracy of inferred labels and image predictions. Note that these functions are already \"jitted\" for performance.\n",
    "\n",
    "Below we simply wrap each of these functions in our training and test loops, respectively. Note that to train in an unsupervised way, you can simply remove the `input` from `jpc.make_pc_step()` and the `evaluate()` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(key, layer_sizes, batch_size, network, test_loader, n_iters):\n",
    "    test_acc = 0\n",
    "    for batch_id, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch = img_batch.numpy()\n",
    "        label_batch = label_batch.numpy()\n",
    "\n",
    "        acc, img_preds = jpc.test_generative_pc(\n",
    "            key=key,\n",
    "            layer_sizes=layer_sizes,\n",
    "            batch_size=batch_size,\n",
    "            network=network,\n",
    "            output=img_batch,\n",
    "            input=label_batch,\n",
    "            n_iters=n_iters\n",
    "        )\n",
    "        test_acc += acc\n",
    "\n",
    "    avg_test_acc = test_acc / len(test_loader)\n",
    "\n",
    "    return avg_test_acc, label_batch, img_preds\n",
    "\n",
    "\n",
    "def train(\n",
    "      key,\n",
    "      layer_sizes,\n",
    "      batch_size,\n",
    "      network,\n",
    "      lr,\n",
    "      n_infer_iters,\n",
    "      test_every,\n",
    "      n_train_iters\n",
    "):\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(network, eqx.is_array))\n",
    "\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "    for iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch = img_batch.numpy()\n",
    "        label_batch = label_batch.numpy()\n",
    "\n",
    "        network, optim, opt_state, train_loss = jpc.make_pc_step(\n",
    "            network=network,\n",
    "            optim=optim,\n",
    "            opt_state=opt_state,\n",
    "            output=img_batch,\n",
    "            input=label_batch,\n",
    "            n_iters=n_infer_iters\n",
    "        )\n",
    "        if ((iter+1) % test_every) == 0:\n",
    "            avg_test_acc, test_label_batch, img_preds = evaluate(\n",
    "                key,\n",
    "                layer_sizes,\n",
    "                batch_size,\n",
    "                network,\n",
    "                test_loader,\n",
    "                n_iters=n_infer_iters\n",
    "            )\n",
    "            print(\n",
    "                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (iter+1) >= n_train_iters:\n",
    "                break\n",
    "\n",
    "    plot_mnist_imgs(img_preds, test_label_batch)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fi69/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/core.py:678: FutureWarning: unhashable type: <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>. Attempting to hash a tracer will lead to an error in a future JAX release.\n",
      "  warnings.warn(\n",
      "/Users/fi69/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/core.py:678: FutureWarning: unhashable type: <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>. Attempting to hash a tracer will lead to an error in a future JAX release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(64,), (1, 10)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/util.py:311\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trace_context_in_key:\n\u001b[0;32m--> 311\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/util.py:304\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:156\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64,), (1, 10)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAYER_SIZES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_infer_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_INFER_ITERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_EVERY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_train_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRAIN_ITERS\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(key, layer_sizes, batch_size, network, lr, n_infer_iters, test_every, n_train_iters)\u001b[0m\n\u001b[1;32m     42\u001b[0m network, optim, opt_state, train_loss \u001b[38;5;241m=\u001b[39m jpc\u001b[38;5;241m.\u001b[39mmake_pc_step(\n\u001b[1;32m     43\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m     44\u001b[0m     optim\u001b[38;5;241m=\u001b[39moptim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     n_iters\u001b[38;5;241m=\u001b[39mn_infer_iters\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m test_every) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     avg_test_acc, test_label_batch, img_preds \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_infer_iters\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg test accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_train_iters:\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(key, layer_sizes, batch_size, network, test_loader, n_iters)\u001b[0m\n\u001b[1;32m      4\u001b[0m     img_batch \u001b[38;5;241m=\u001b[39m img_batch\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m     label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 7\u001b[0m     acc, img_preds \u001b[38;5;241m=\u001b[39m \u001b[43mjpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_generative_pc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     test_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m     18\u001b[0m avg_test_acc \u001b[38;5;241m=\u001b[39m test_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/jpc/_test.py:105\u001b[0m, in \u001b[0;36mtest_generative_pc\u001b[0;34m(key, layer_sizes, batch_size, network, output, input, sigma, solver, dt, n_iters, stepsize_controller)\u001b[0m\n\u001b[1;32m     89\u001b[0m activities \u001b[38;5;241m=\u001b[39m init_activities_from_gaussian(\n\u001b[1;32m     90\u001b[0m     key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m     91\u001b[0m     layer_sizes\u001b[38;5;241m=\u001b[39mlayer_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     sigma\u001b[38;5;241m=\u001b[39msigma\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     96\u001b[0m equilib_activities \u001b[38;5;241m=\u001b[39m solve_pc_activities(\n\u001b[1;32m     97\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m     98\u001b[0m     activities\u001b[38;5;241m=\u001b[39mactivities,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     dt\u001b[38;5;241m=\u001b[39mdt\n\u001b[1;32m    104\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m input_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequilib_activities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m output_preds \u001b[38;5;241m=\u001b[39m init_activities_with_ffwd(network\u001b[38;5;241m=\u001b[39mnetwork, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_acc, output_preds\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/jpc/_utils.py:77\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(truths, preds)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(truths: ArrayLike, preds: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Scalar:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean(\n\u001b[0;32m---> 77\u001b[0m         \u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:739\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 739\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:265\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:86\u001b[0m, in \u001b[0;36m_one_to_one_binop.<locals>.<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     84\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39mpromote_args_numeric(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     88\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.numpy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:381\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    379\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    380\u001b[0m check_for_prngkeys(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:250\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    249\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 250\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64,), (1, 10)]"
     ]
    }
   ],
   "source": [
    "network = train(\n",
    "    key=key,\n",
    "    layer_sizes=LAYER_SIZES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    network=network,\n",
    "    lr=LEARNING_RATE,\n",
    "    n_infer_iters=N_INFER_ITERS,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
