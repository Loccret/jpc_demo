{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# μPC\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thebuckleylab/jpc/blob/main/examples/mupc.ipynb)\n",
    "\n",
    "This notebook demonstrates how to train residual networks with [**μPC**](https://arxiv.org/abs/2505.13124), a reparameterisation of PC that allows stable training of very deep (100+ layer) networks while also enabling zero-shot hyperparameter transfer (see [Innocenti et al., 2025](https://arxiv.org/abs/2505.13124))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install torch==2.3.1\n",
    "# !pip install torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Callable\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')  # ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "def set_global_seed(seed):\n",
    "    torch.manual_seed(seed)             \n",
    "    torch.cuda.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)        \n",
    "    np.random.seed(seed)                  \n",
    "    random.seed(seed)                       \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including the network architecture, learning rate, batch size, etc. We choose a network with \"only\" 30 layers and 128 hidden neurons so that it can run relatively fast on a CPU, but feel free to try deeper and wider networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4329\n",
    "\n",
    "INPUT_DIM = 784\n",
    "WIDTH = 128\n",
    "DEPTH = 30\n",
    "OUTPUT_DIM = 10\n",
    "ACT_FN = \"relu\"\n",
    "\n",
    "ACTIVITY_LR = 5e-1\n",
    "PARAM_LR = 1e-1\n",
    "BATCH_SIZE = 64\n",
    "TEST_EVERY = 100\n",
    "N_TRAIN_ITERS = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a μPC model\n",
    "\n",
    "To parameterise a model with μPC, one can use a few convenience functions of `jpc` to create an MLP or fully connected network with [`jpc.make_mlp()`](https://thebuckleylab.github.io/jpc/api/Utils/#jpc.make_mlp) and an associated skip model with [`jpc.make_skip model()`](https://thebuckleylab.github.io/jpc/api/Utils/#jpc.make_skip_model). Note that μPC works only for a specific type of ResNet, namely one with one-layer skip connections at every layer except from the input to the next layer and from the penultimate layer to the output (see [Innocenti et al., 2025](https://arxiv.org/abs/2505.13124)), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-31 20:43:07,231:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(SEED)\n",
    "\n",
    "# MLP\n",
    "model = jpc.make_mlp(\n",
    "    key,\n",
    "    input_dim=INPUT_DIM,\n",
    "    width=WIDTH,\n",
    "    depth=DEPTH,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    act_fn=ACT_FN,\n",
    "    param_type=\"mupc\"\n",
    ")\n",
    "\n",
    "# skip model\n",
    "skip_model = jpc.make_skip_model(DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At training and test time we would need to pass both models to relevant `jpc` functions and change the argument `param_type = \"mupc\"` (default is `\"sp\"` for standard parameterisation). \n",
    "\n",
    "Alternatively, one could define a model class embedding the parameterisation itself and leave the above arguments to their default. This solution is more elegant but it can be harder to debug, at least for a fully connected architecture. However, if you would like to experiment with different parameterisations and more complex architectures (e.g. CNNs), we recommend this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledLinear(eqx.Module):\n",
    "    \"\"\"Scaled linear transformation.\"\"\"\n",
    "    linear: nn.Linear\n",
    "    scaling: float = eqx.static_field()\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            out_features,\n",
    "            *,\n",
    "            key,\n",
    "            scaling=1.,\n",
    "            param_type=\"sp\",\n",
    "            use_bias=False\n",
    "    ):\n",
    "        keys = jr.split(key, 2)\n",
    "        linear = nn.Linear(\n",
    "            in_features, \n",
    "            out_features, \n",
    "            use_bias=use_bias,\n",
    "            key=keys[0]\n",
    "        )\n",
    "        if param_type == \"mupc\":\n",
    "            W = jr.normal(keys[1], linear.weight.shape)\n",
    "            linear = eqx.tree_at(lambda l: l.weight, linear, W)\n",
    "\n",
    "        self.linear = linear\n",
    "        self.scaling = scaling\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.scaling * self.linear(x)\n",
    "        \n",
    "\n",
    "class ResNetBlock(eqx.Module):\n",
    "    \"\"\"Identity residual block applying activation and a scaled linear layer.\"\"\"\n",
    "    act_fn: Callable = eqx.static_field()\n",
    "    scaled_linear: ScaledLinear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        *,\n",
    "        key,\n",
    "        scaling=1.,\n",
    "        param_type=\"sp\",\n",
    "        use_bias=False,\n",
    "        act_fn=\"linear\"\n",
    "    ):\n",
    "        self.act_fn = act_fn\n",
    "        self.scaled_linear = ScaledLinear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            key=key,\n",
    "            scaling=scaling,\n",
    "            param_type=param_type,\n",
    "            use_bias=use_bias\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        res_path = x\n",
    "        x = self.act_fn(x)\n",
    "        return self.scaled_linear(x) + res_path\n",
    "\n",
    "\n",
    "class Readout(eqx.Module):\n",
    "    \"\"\"Final network layer applying activation and a scaled linear layer.\"\"\"\n",
    "    act_fn: Callable = eqx.static_field()\n",
    "    scaled_linear: ScaledLinear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        *,\n",
    "        key,\n",
    "        scaling=1.,\n",
    "        param_type=\"sp\",\n",
    "        use_bias=False,\n",
    "        act_fn=\"linear\"\n",
    "    ):\n",
    "        self.act_fn = act_fn\n",
    "        self.scaled_linear = ScaledLinear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            key=key,\n",
    "            scaling=scaling,\n",
    "            param_type=param_type,\n",
    "            use_bias=use_bias\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.act_fn(x)\n",
    "        return self.scaled_linear(x)\n",
    "\n",
    "\n",
    "class FCResNet(eqx.Module):\n",
    "    \"\"\"Fully-connected ResNet compatible with different parameterisations.\"\"\"\n",
    "    layers: List[eqx.Module]\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            *,\n",
    "            key, \n",
    "            in_dim, \n",
    "            width, \n",
    "            depth, \n",
    "            out_dim, \n",
    "            act_fn=\"linear\", \n",
    "            use_bias=False,\n",
    "            param_type=\"sp\"\n",
    "        ):\n",
    "        act_fn = jpc.get_act_fn(act_fn)\n",
    "        if param_type == \"sp\":\n",
    "            in_scaling = 1.\n",
    "            hidden_scaling = 1.\n",
    "            out_scaling = 1.\n",
    "        \n",
    "        elif param_type == \"mupc\":\n",
    "            in_scaling = 1 / math.sqrt(in_dim)\n",
    "            hidden_scaling = 1 / math.sqrt(width * depth)\n",
    "            out_scaling = 1 / width\n",
    "            \n",
    "        keys = jr.split(key, depth)\n",
    "        self.layers = [\n",
    "            ScaledLinear(\n",
    "                key=keys[0],\n",
    "                in_features=in_dim,\n",
    "                out_features=width,\n",
    "                scaling=in_scaling,\n",
    "                param_type=param_type,\n",
    "                use_bias=use_bias\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        for i in range(1, depth - 1):\n",
    "            self.layers.append(\n",
    "                ResNetBlock(\n",
    "                    key=keys[i],\n",
    "                    in_features=width,\n",
    "                    out_features=width,\n",
    "                    scaling=hidden_scaling,\n",
    "                    param_type=param_type,\n",
    "                    use_bias=use_bias,\n",
    "                    act_fn=act_fn\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layers.append(\n",
    "            Readout(\n",
    "                key=keys[-1],\n",
    "                in_features=width,\n",
    "                out_features=out_dim,\n",
    "                scaling=out_scaling,\n",
    "                param_type=param_type,\n",
    "                use_bias=use_bias,\n",
    "                act_fn=act_fn\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self.layers:\n",
    "            x = f(x)      \n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupc_model = FCResNet(\n",
    "    key=key, \n",
    "    in_dim=INPUT_DIM, \n",
    "    width=WIDTH, \n",
    "    depth=DEPTH, \n",
    "    out_dim=OUTPUT_DIM, \n",
    "    act_fn=ACT_FN, \n",
    "    use_bias=False, \n",
    "    param_type=\"mupc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following makes sure that the models have identical weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mupc_model = FCResNet(\n",
    "#     key=key, \n",
    "#     in_dim=INPUT_DIM, \n",
    "#     width=WIDTH, \n",
    "#     depth=DEPTH, \n",
    "#     out_dim=OUTPUT_DIM, \n",
    "#     act_fn=ACT_FN, \n",
    "#     use_bias=False, \n",
    "#     param_type=\"mupc\"\n",
    "# )\n",
    "# mupc_model = eqx.tree_at(\n",
    "#     where=lambda tree: tree[0].linear.weight,\n",
    "#     pytree=mupc_model,\n",
    "#     replace=model[0][1].weight\n",
    "# )\n",
    "# for l in range(1, len(model)):\n",
    "#     mupc_model = eqx.tree_at(\n",
    "#         where=lambda tree: tree[l].scaled_linear.linear.weight,\n",
    "#         pytree=mupc_model,\n",
    "#         replace=model[l][1].weight\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test\n",
    "\n",
    "For training, we use the [advanced API](https://thebuckleylab.github.io/jpc/advanced_usage/) including the functions [`jpc.init_activities_with_ffwd()`](https://thebuckleylab.github.io/jpc/api/Initialisation/#jpc.init_activities_with_ffwd) to initialise the activities, [`jpc.update_activities()`](https://thebuckleylab.github.io/jpc/api/Discrete%20updates/#jpc.update_activities) to perform PC inference, and [`jpc.update_params()`](https://thebuckleylab.github.io/jpc/api/Discrete%20updates/#jpc.update_params) to update the weights. All these functions accept `skip_model` and `param_type` as arguments. Note, however, that one can replace these functions with [`jpc.make_pc_step()`](https://thebuckleylab.github.io/jpc/api/Training/#jpc.make_pc_step). For testing, we use [`jpc.test_discriminative_pc()`](https://thebuckleylab.github.io/jpc/api/Testing/#jpc.test_discriminative_pc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, skip_model, test_loader, param_type):\n",
    "    avg_test_acc = 0\n",
    "    for _, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        _, test_acc = jpc.test_discriminative_pc(\n",
    "            model=model,\n",
    "            input=img_batch,\n",
    "            output=label_batch,\n",
    "            skip_model=skip_model,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        avg_test_acc += test_acc\n",
    "\n",
    "    return avg_test_acc / len(test_loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "      seed,  \n",
    "      model,\n",
    "      skip_model,\n",
    "      param_type,\n",
    "      activity_lr,  \n",
    "      param_lr,\n",
    "      batch_size,\n",
    "      test_every,\n",
    "      n_train_iters\n",
    "):  \n",
    "    set_global_seed(seed)\n",
    "    activity_optim = optax.sgd(activity_lr)\n",
    "    param_optim = optax.adam(param_lr)\n",
    "    param_opt_state = param_optim.init(\n",
    "        (eqx.filter(model, eqx.is_array), skip_model)\n",
    "    )\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "    for iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        # initialise activities\n",
    "        activities = jpc.init_activities_with_ffwd(\n",
    "            model=model,\n",
    "            input=img_batch,\n",
    "            skip_model=skip_model,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        activity_opt_state = activity_optim.init(activities)\n",
    "        train_loss = jpc.mse_loss(activities[-1], label_batch)\n",
    "\n",
    "        # inference\n",
    "        for t in range(len(model)):\n",
    "            activity_update_result = jpc.update_activities(\n",
    "                params=(model, skip_model),\n",
    "                activities=activities,\n",
    "                optim=activity_optim,\n",
    "                opt_state=activity_opt_state,\n",
    "                output=label_batch,\n",
    "                input=img_batch,\n",
    "                param_type=param_type\n",
    "            )\n",
    "            activities = activity_update_result[\"activities\"]\n",
    "            activity_opt_state = activity_update_result[\"opt_state\"]\n",
    "\n",
    "        # learning\n",
    "        param_update_result = jpc.update_params(\n",
    "            params=(model, skip_model),\n",
    "            activities=activities,\n",
    "            optim=param_optim,\n",
    "            opt_state=param_opt_state,\n",
    "            output=label_batch,\n",
    "            input=img_batch,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        model = param_update_result[\"model\"]\n",
    "        skip_model = param_update_result[\"skip_model\"]\n",
    "        param_opt_state = param_update_result[\"opt_state\"]\n",
    "\n",
    "        if np.isinf(train_loss) or np.isnan(train_loss):\n",
    "            print(\n",
    "                f\"Stopping training because of divergence, train loss={train_loss}\"\n",
    "            )\n",
    "            break\n",
    "    \n",
    "        if ((iter+1) % test_every) == 0:\n",
    "            avg_test_acc = evaluate(\n",
    "                model=model,\n",
    "                skip_model=skip_model, \n",
    "                test_loader=test_loader, \n",
    "                param_type=param_type\n",
    "            )\n",
    "            print(\n",
    "                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (iter+1) >= n_train_iters:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on a CPU the script below should take about a minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 100, train loss=0.016440, avg test accuracy=85.947517\n",
      "Train iter 200, train loss=0.012121, avg test accuracy=88.641830\n",
      "Train iter 300, train loss=0.009404, avg test accuracy=90.594955\n",
      "Train iter 400, train loss=0.009140, avg test accuracy=91.356171\n",
      "Train iter 500, train loss=0.011321, avg test accuracy=92.097359\n",
      "Train iter 600, train loss=0.007968, avg test accuracy=92.087341\n",
      "Train iter 700, train loss=0.007693, avg test accuracy=92.788460\n",
      "Train iter 800, train loss=0.009208, avg test accuracy=92.978767\n",
      "Train iter 900, train loss=0.009734, avg test accuracy=93.579727\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    seed=SEED,\n",
    "    model=model,\n",
    "    skip_model=skip_model,\n",
    "    param_type=\"mupc\",\n",
    "    activity_lr=ACTIVITY_LR,\n",
    "    param_lr=PARAM_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, try to change to the standard parameterisation with `param_type = \"sp\"`. \n",
    "\n",
    "If you are using your own μPC-parameterised model class, then you can leave the default `skip_model = None` and `param_type = \"sp\"`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 100, train loss=0.016874, avg test accuracy=84.615387\n",
      "Train iter 200, train loss=0.012407, avg test accuracy=88.661858\n",
      "Train iter 300, train loss=0.011173, avg test accuracy=91.115784\n",
      "Train iter 400, train loss=0.010616, avg test accuracy=91.356171\n",
      "Train iter 500, train loss=0.012811, avg test accuracy=92.227562\n",
      "Train iter 600, train loss=0.008681, avg test accuracy=92.748398\n",
      "Train iter 700, train loss=0.007163, avg test accuracy=93.289261\n",
      "Train iter 800, train loss=0.010214, avg test accuracy=92.958733\n",
      "Train iter 900, train loss=0.010398, avg test accuracy=92.858574\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    seed=SEED,\n",
    "    model=mupc_model,\n",
    "    skip_model=None,\n",
    "    param_type=\"sp\",\n",
    "    activity_lr=ACTIVITY_LR,\n",
    "    param_lr=PARAM_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
