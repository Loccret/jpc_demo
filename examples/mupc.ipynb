{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# μPC\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thebuckleylab/jpc/blob/main/examples/mupc.ipynb)\n",
    "\n",
    "This notebook demonstrates how to train residual networks with [**μPC**](https://arxiv.org/abs/2505.13124), a reparameterisation of PC that allows stable training of very deep (100+ layer) networks while also enabling zero-shot hyperparameter transfer (see [Innocenti et al., 2025](https://arxiv.org/abs/2505.13124))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "from jax.tree_util import tree_leaves\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')  # ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including the network architecture, learning rate, batch size, etc. We choose a network with \"only\" 30 layers and 128 hidden neurons so that it can run relatively fast on a CPU, but feel free to try deeper and wider networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "INPUT_DIM = 784\n",
    "WIDTH = 128\n",
    "DEPTH = 30\n",
    "OUTPUT_DIM = 10\n",
    "ACT_FN = \"relu\"\n",
    "\n",
    "ACTIVITY_LR = 5e-1\n",
    "PARAM_LR = 1e-1\n",
    "BATCH_SIZE = 64\n",
    "TEST_EVERY = 100\n",
    "N_TRAIN_ITERS = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a μPC model\n",
    "\n",
    "To parameterise a model with μPC, one can use a few convenience functions of `jpc` to create an MLP or fully connected network with [`jpc.make_mlp()`](https://thebuckleylab.github.io/jpc/api/Utils/#jpc.make_mlp) and an associated skip model with [`jpc.make_skip model()`](https://thebuckleylab.github.io/jpc/api/Utils/#jpc.make_skip_model). Note that μPC works only for a specific type of ResNet, namely one with one-layer skip connections at every layer except from the input to the next layer and from the penultimate layer to the output (see [Innocenti et al., 2025](https://arxiv.org/abs/2505.13124)), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(SEED)\n",
    "\n",
    "# MLP\n",
    "model = jpc.make_mlp(\n",
    "    key,\n",
    "    input_dim=INPUT_DIM,\n",
    "    width=WIDTH,\n",
    "    depth=DEPTH,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    act_fn=ACT_FN,\n",
    "    param_type=\"mupc\"\n",
    ")\n",
    "\n",
    "# skip model\n",
    "skip_model = jpc.make_skip_model(DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At training and test time we would need to pass both models to relevant `jpc` functions and change the argument `param_type = \"mupc\"` (default is `\"sp\"` for standard parameterisation). \n",
    "\n",
    "Alternatively, one could define a model class embedding the parameterisation itself and leave the above arguments to their default. This solution is more elegant but it can be harder to debug, at least for a fully connected architecture. However, if you would like to experiment with different parameterisations and more complex architectures (e.g. CNNs), we recommend this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test\n",
    "\n",
    "For training, we use the [advanced API](https://thebuckleylab.github.io/jpc/advanced_usage/) including the functions [`jpc.init_activities_with_ffwd()`](https://thebuckleylab.github.io/jpc/api/Initialisation/#jpc.init_activities_with_ffwd) to initialise the activities, [`jpc.update_activities()`](https://thebuckleylab.github.io/jpc/api/Discrete%20updates/#jpc.update_activities) to perform PC inference, and [`jpc.update_params()`](https://thebuckleylab.github.io/jpc/api/Discrete%20updates/#jpc.update_params) to update the weights. All these functions accept `skip_model` and `param_type` as arguments. Note, however, that one can replace these functions with [`jpc.make_pc_step()`](https://thebuckleylab.github.io/jpc/api/Training/#jpc.make_pc_step). For testing, we use [`jpc.test_discriminative_pc()`](https://thebuckleylab.github.io/jpc/api/Testing/#jpc.test_discriminative_pc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, skip_model, test_loader, param_type):\n",
    "    avg_test_acc = 0\n",
    "    for _, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        _, test_acc = jpc.test_discriminative_pc(\n",
    "            model=model,\n",
    "            input=img_batch,\n",
    "            output=label_batch,\n",
    "            skip_model=skip_model,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        avg_test_acc += test_acc\n",
    "\n",
    "    return avg_test_acc / len(test_loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "      model,\n",
    "      skip_model,\n",
    "      param_type,\n",
    "      activity_lr,  \n",
    "      param_lr,\n",
    "      batch_size,\n",
    "      test_every,\n",
    "      n_train_iters\n",
    "):  \n",
    "    activity_optim = optax.sgd(activity_lr)\n",
    "    param_optim = optax.adam(param_lr)\n",
    "    param_opt_state = param_optim.init(\n",
    "        (eqx.filter(model, eqx.is_array), skip_model)\n",
    "    )\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "    for iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        # initialise activities\n",
    "        activities = jpc.init_activities_with_ffwd(\n",
    "            model=model,\n",
    "            input=img_batch,\n",
    "            skip_model=skip_model,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        activity_opt_state = activity_optim.init(activities)\n",
    "        train_loss = jpc.mse_loss(activities[-1], label_batch)\n",
    "\n",
    "        # inference\n",
    "        for t in range(len(model)):\n",
    "            activity_update_result = jpc.update_activities(\n",
    "                params=(model, skip_model),\n",
    "                activities=activities,\n",
    "                optim=activity_optim,\n",
    "                opt_state=activity_opt_state,\n",
    "                output=label_batch,\n",
    "                input=img_batch,\n",
    "                param_type=param_type\n",
    "            )\n",
    "            activities = activity_update_result[\"activities\"]\n",
    "            activity_opt_state = activity_update_result[\"opt_state\"]\n",
    "\n",
    "        # learning\n",
    "        param_update_result = jpc.update_params(\n",
    "            params=(model, skip_model),\n",
    "            activities=activities,\n",
    "            optim=param_optim,\n",
    "            opt_state=param_opt_state,\n",
    "            output=label_batch,\n",
    "            input=img_batch,\n",
    "            param_type=param_type\n",
    "        )\n",
    "        model = param_update_result[\"model\"]\n",
    "        skip_model = param_update_result[\"skip_model\"]\n",
    "        param_opt_state = param_update_result[\"opt_state\"]\n",
    "\n",
    "        if np.isinf(train_loss) or np.isnan(train_loss):\n",
    "            print(\n",
    "                f\"Stopping training because of divergence, train loss={train_loss}\"\n",
    "            )\n",
    "            break\n",
    "    \n",
    "        if ((iter+1) % test_every) == 0:\n",
    "            avg_test_acc = evaluate(\n",
    "                model=model,\n",
    "                skip_model=skip_model, \n",
    "                test_loader=test_loader, \n",
    "                param_type=param_type\n",
    "            )\n",
    "            print(\n",
    "                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (iter+1) >= n_train_iters:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on a CPU the script below should take about a minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 100, train loss=0.014857, avg test accuracy=86.718750\n",
      "Train iter 200, train loss=0.012227, avg test accuracy=89.352966\n",
      "Train iter 300, train loss=0.012027, avg test accuracy=91.065704\n",
      "Train iter 400, train loss=0.010815, avg test accuracy=91.766830\n",
      "Train iter 500, train loss=0.010547, avg test accuracy=92.497993\n",
      "Train iter 600, train loss=0.008235, avg test accuracy=92.738380\n",
      "Train iter 700, train loss=0.009772, avg test accuracy=93.199120\n",
      "Train iter 800, train loss=0.010498, avg test accuracy=93.699921\n",
      "Train iter 900, train loss=0.009273, avg test accuracy=93.509613\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    skip_model=skip_model,\n",
    "    param_type=\"mupc\",\n",
    "    activity_lr=ACTIVITY_LR,\n",
    "    param_lr=PARAM_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, try to change to the standard parameterisation with `param_type = \"sp\"`. If you are using your own μPC-parameterised model class, then you can leave the default `skip_model = None` and `param_type = \"sp\"`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
