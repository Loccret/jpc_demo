{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical PC energy of deep linear networks\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thebuckleylab/jpc/blob/main/examples/theoretical_energy_with_linear_net.ipynb)\n",
    "\n",
    "This notebook demonstrates how to compute the theoretical PC energy at the inference equilibrium $\\mathcal{F}(\\mathbf{z}^*)$ when $\\mathcal{F}|_{\\nabla_{\\mathbf{z}} \\mathcal{F} = \\mathbf{0}}$ for a deep linear network with input and output $(\\mathbf{x}_i, \\mathbf{y}_i)$ (see [Innocenti et al., 2024](https://papers.nips.cc/paper_files/paper/2024/hash/6075fc6540b9a3cb951752099efd86ef-Abstract-Conference.html))\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{F}(\\mathbf{z}^*) = \\frac{1}{2N} \\sum_{i=1}^N (\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)^T \\mathbf{S}^{-1}(\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{S} = \\mathbf{I}_{d_y} + \\sum_{\\ell=2}^L (\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^T$ and $\\mathbf{W}_{k:\\ell} = \\mathbf{W}_k \\dots \\mathbf{W}_\\ell$ for $\\ell, k \\in 1,\\dots, L$. Note that this result can be generalised to any linear layer transformation $\\mathbf{B}_\\ell$, e.g. for a ResNet $\\mathbf{B}_\\ell = \\mathbf{I} + \\mathbf{W}_\\ell$ (see [Innocenti et al., 2025](https://arxiv.org/abs/2505.13124)). However, currently `jpc` only provides a theoretical energy for standard fully connected linear nets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install plotly==5.11.0\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')  # ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including network architecture, learning rate, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "INPUT_DIM = 784\n",
    "WIDTH = 300\n",
    "DEPTH = 10\n",
    "OUTPUT_DIM = 10\n",
    "ACT_FN = \"linear\"\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "MAX_T1 = 300  # 300\n",
    "TEST_EVERY = 10\n",
    "N_TRAIN_ITERS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title data utils\n",
    "\n",
    "\n",
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_energies(energies):\n",
    "    n_train_iters = len(energies[\"theory\"])\n",
    "    train_iters = [b+1 for b in range(n_train_iters)]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for energy_type, energy in energies.items():\n",
    "        is_theory = energy_type == \"theory\"\n",
    "        fig.add_traces(\n",
    "            go.Scatter(\n",
    "                x=train_iters,\n",
    "                y=energy,\n",
    "                name=energy_type,\n",
    "                mode=\"lines\",\n",
    "                line=dict(\n",
    "                    width=3, \n",
    "                    dash=\"dash\" if is_theory else \"solid\",\n",
    "                    color=\"rgb(27, 158, 119)\" if is_theory else \"#00CC96\"\n",
    "                ),\n",
    "                legendrank=1 if is_theory else 2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300,\n",
    "        width=450,\n",
    "        xaxis=dict(\n",
    "            title=\"Training iteration\",\n",
    "            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n",
    "            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]],\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Energy\",\n",
    "            nticks=3\n",
    "        ),\n",
    "        font=dict(size=16),\n",
    "    )\n",
    "    fig.write_image(\"dln_theory_energy.pdf\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear network\n",
    "\n",
    "We'll use a linear network with 10 hidden layers as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "width, n_hidden = 300, 10\n",
    "network = jpc.make_mlp(\n",
    "    key, \n",
    "    input_dim=INPUT_DIM,\n",
    "    width=WIDTH,\n",
    "    depth=DEPTH,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    act_fn=jpc.get_act_fn(ACT_FN),\n",
    "    use_bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train and test\n",
    "\n",
    "To compute the theoretical energy, we can use `jpc.compute_linear_equilib_energy()` (see the the [docs](https://thebuckleylab.github.io/jpc/api/Analytical%20tools/#jpc.compute_linear_equilib_energy) for more details) which as clear from the equation above just takes some linear network and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    avg_test_loss, avg_test_acc = 0, 0\n",
    "    for batch_id, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        test_loss, test_acc = jpc.test_discriminative_pc(\n",
    "            model=model,\n",
    "            output=label_batch,\n",
    "            input=img_batch\n",
    "        )\n",
    "        avg_test_loss += test_loss\n",
    "        avg_test_acc += test_acc\n",
    "\n",
    "    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n",
    "\n",
    "\n",
    "def train( \n",
    "      model,  \n",
    "      lr,\n",
    "      batch_size,\n",
    "      max_t1,\n",
    "      test_every,\n",
    "      n_train_iters\n",
    "):\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(\n",
    "        (eqx.filter(model, eqx.is_array), None)\n",
    "    )\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "    num_total_energies, theory_total_energies = [], []\n",
    "    for iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        theory_total_energies.append(\n",
    "            jpc.compute_linear_equilib_energy(\n",
    "                network=model, \n",
    "                x=img_batch, \n",
    "                y=label_batch\n",
    "            )\n",
    "        )\n",
    "        result = jpc.make_pc_step(\n",
    "            model,\n",
    "            optim,\n",
    "            opt_state,\n",
    "            output=label_batch,\n",
    "            input=img_batch,\n",
    "            max_t1=max_t1,\n",
    "            record_energies=True\n",
    "        )\n",
    "        model, opt_state = result[\"model\"], result[\"opt_state\"]\n",
    "        train_loss, t_max = result[\"loss\"], result[\"t_max\"]\n",
    "        num_total_energies.append(result[\"energies\"][:, t_max-1].sum())\n",
    "\n",
    "        if ((iter+1) % test_every) == 0:\n",
    "            avg_test_loss, avg_test_acc = evaluate(model, test_loader)\n",
    "            print(\n",
    "                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (iter+1) >= n_train_iters:\n",
    "                break\n",
    "    \n",
    "    return {\n",
    "        \"experiment\": jnp.array(num_total_energies),\n",
    "        \"theory\": jnp.array(theory_total_energies)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Below we plot the theoretical energy against the numerical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 10, train loss=0.037435, avg test accuracy=55.699120\n",
      "Train iter 20, train loss=0.033419, avg test accuracy=65.735176\n",
      "Train iter 30, train loss=0.030995, avg test accuracy=74.218750\n",
      "Train iter 40, train loss=0.027386, avg test accuracy=75.570915\n",
      "Train iter 50, train loss=0.024435, avg test accuracy=77.343750\n",
      "Train iter 60, train loss=0.026050, avg test accuracy=79.807693\n",
      "Train iter 70, train loss=0.027464, avg test accuracy=81.480370\n",
      "Train iter 80, train loss=0.030156, avg test accuracy=79.977966\n",
      "Train iter 90, train loss=0.025296, avg test accuracy=80.719154\n",
      "Train iter 100, train loss=0.026032, avg test accuracy=80.709137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"470px\"\n",
       "    height=\"320\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "energies = train(\n",
    "    model=network,\n",
    "    lr=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    max_t1=MAX_T1,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")\n",
    "plot_total_energies(energies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
