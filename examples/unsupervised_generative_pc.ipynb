{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised generative PC on MNIST\n",
    "\n",
    "This notebook demonstrates how to train a neural network with predictive coding to encode MNIST digits in an unsupervised manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install matplotlib==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including network architecture, learning rate, batch size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "LAYER_SIZES = [10, 300, 300, 784]\n",
    "ACT_FN = \"relu\"\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "N_INFER_ITERS = 100\n",
    "N_TRAIN_ITERS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch and plot MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title data utils\n",
    "\n",
    "\n",
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_energies(train_energies, ts):\n",
    "    t_max = int(ts[0])\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=len(energies)-1)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    cmap_blues = plt.get_cmap(\"Blues\")\n",
    "    cmap_reds = plt.get_cmap(\"Reds\")\n",
    "    cmap_greens = plt.get_cmap(\"Greens\")\n",
    "    \n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    for t, energies_iter in enumerate(energies):\n",
    "        line1, = ax.plot(energies_iter[0, :t_max], color=cmap_blues(norm(t)))\n",
    "        line2, = ax.plot(energies_iter[1, :t_max], color=cmap_reds(norm(t)))\n",
    "        line3, = ax.plot(energies_iter[2, :t_max], color=cmap_greens(norm(t)))\n",
    "    \n",
    "        if t == 70:\n",
    "            legend_handles.append(line1)\n",
    "            legend_labels.append(\"$\\ell_1$\")\n",
    "            legend_handles.append(line2)\n",
    "            legend_labels.append(\"$\\ell_2$\")\n",
    "            legend_handles.append(line3)\n",
    "            legend_labels.append(\"$\\ell_3$\")\n",
    "    \n",
    "    ax.legend(legend_handles, legend_labels, loc=\"best\", fontsize=16)\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap(\"Greys\"), norm=norm)\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"Training iteration\", fontsize=16, labelpad=14)\n",
    "    cbar.ax.tick_params(labelsize=14) \n",
    "    plt.gca().tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "    \n",
    "    ax.set_xlabel(\"Inference iterations\", fontsize=18, labelpad=14)\n",
    "    ax.set_ylabel(\"Energy\", fontsize=18, labelpad=14)\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Network\n",
    "\n",
    "For `jpc` to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like `nn.Sequential()` in [Equinox](https://github.com/patrick-kidger/equinox). For example, we can define a ReLU MLP with two hidden layers as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,10],\n",
      "      bias=f32[300],\n",
      "      in_features=10,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,300],\n",
      "      bias=f32[300],\n",
      "      in_features=300,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Linear(\n",
      "  weight=f32[784,300],\n",
      "  bias=f32[784],\n",
      "  in_features=300,\n",
      "  out_features=784,\n",
      "  use_bias=True\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(SEED)\n",
    "key, *subkeys = jax.random.split(key, 4)\n",
    "network = [\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(10, 300, key=subkeys[0]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(300, 300, key=subkeys[1]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Linear(300, 784, key=subkeys[2]),\n",
    "]\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the utility `jpc.get_fc_network` to define an MLP or fully connected network with some activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,10],\n",
      "      bias=f32[300],\n",
      "      in_features=10,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Linear(\n",
      "      weight=f32[300,300],\n",
      "      bias=f32[300],\n",
      "      in_features=300,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    Lambda(fn=<wrapped function relu>)\n",
      "  )\n",
      "), Linear(\n",
      "  weight=f32[784,300],\n",
      "  bias=f32[784],\n",
      "  in_features=300,\n",
      "  out_features=784,\n",
      "  use_bias=True\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "network = jpc.get_fc_network(key, LAYER_SIZES, act_fn=\"relu\")\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "      key,\n",
    "      layer_sizes,\n",
    "      batch_size,\n",
    "      network,\n",
    "      lr,\n",
    "      n_infer_iters,\n",
    "      n_train_iters\n",
    "):\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(network, eqx.is_array))\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "    train_energies, ts = [], []\n",
    "    for iter, img_batch in enumerate(train_loader):\n",
    "        img_batch = img_batch.numpy()\n",
    "\n",
    "        result = jpc.make_pc_step(\n",
    "            key=key,\n",
    "            layer_sizes=layer_sizes,\n",
    "            batch_size=batch_size,\n",
    "            network=network,\n",
    "            optim=optim,\n",
    "            opt_state=opt_state,\n",
    "            output=img_batch,\n",
    "            n_iters=n_infer_iters,\n",
    "            record_activities=True,\n",
    "            record_energies=True\n",
    "        )\n",
    "        network, optim, opt_state = result[\"network\"], result[\"optim\"], result[\"opt_state\"]\n",
    "        train_energies.append(result[\"energies\"])\n",
    "        ts.append(result[\"t_max\"])\n",
    "        if (iter+1) >= n_train_iters:\n",
    "            break\n",
    "\n",
    "    return result[\"network\"], train_energies, ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fi69/PycharmProjects/jpc/venv/lib/python3.10/site-packages/jax/_src/core.py:678: FutureWarning: unhashable type: <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>. Attempting to hash a tracer will lead to an error in a future JAX release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "network, energies, ts = train(\n",
    "    key=key,\n",
    "    layer_sizes=LAYER_SIZES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    network=network,\n",
    "    lr=LEARNING_RATE,\n",
    "    n_infer_iters=N_INFER_ITERS,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")\n",
    "plot_train_energies(energies, ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
