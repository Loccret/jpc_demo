{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative PC on MNIST\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thebuckleylab/jpc/blob/main/examples/discriminative_pc.ipynb)\n",
    "\n",
    "This notebook demonstrates how to train a simple feedforward network with predictive coding (PC) to discriminate or classify MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install torch==2.3.1\n",
    "# !pip install torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpc\n",
    "\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')  # ignore warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters, including the network architecture, learning rate, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "INPUT_DIM = 784\n",
    "WIDTH = 300\n",
    "DEPTH = 3\n",
    "OUTPUT_DIM = 10\n",
    "ACT_FN = \"relu\"\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "TEST_EVERY = 100\n",
    "N_TRAIN_ITERS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Network\n",
    "\n",
    "For `jpc` to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like [`nn.Sequential()`](https://docs.kidger.site/equinox/api/nn/sequential/#equinox.nn.Sequential) in [equinox](https://github.com/patrick-kidger/equinox). For example, we can define a ReLU MLP with two hidden layers as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-28 14:39:35,628:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(SEED)\n",
    "_, *subkeys = jax.random.split(key, 4)\n",
    "network = [\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(784, 300, key=subkeys[0]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(300, 300, key=subkeys[1]),\n",
    "            nn.Lambda(jax.nn.relu)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Linear(300, 10, key=subkeys[2]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use [`jpc.make_mlp()`](https://thebuckleylab.github.io/jpc/api/Utils/#jpc.make_mlp) to define a multi-layer perceptron (MLP) or fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  layers=(\n",
      "    Lambda(fn=Identity()),\n",
      "    Linear(\n",
      "      weight=f32[300,784],\n",
      "      bias=f32[300],\n",
      "      in_features=784,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    )\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Lambda(fn=<PjitFunction of <function relu at 0x77699a43ec00>>),\n",
      "    Linear(\n",
      "      weight=f32[300,300],\n",
      "      bias=f32[300],\n",
      "      in_features=300,\n",
      "      out_features=300,\n",
      "      use_bias=True\n",
      "    )\n",
      "  )\n",
      "), Sequential(\n",
      "  layers=(\n",
      "    Lambda(fn=<PjitFunction of <function relu at 0x77699a43ec00>>),\n",
      "    Linear(\n",
      "      weight=f32[10,300],\n",
      "      bias=f32[10],\n",
      "      in_features=300,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    )\n",
      "  )\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "network = jpc.make_mlp(\n",
    "    key,\n",
    "    input_dim=INPUT_DIM,\n",
    "    width=WIDTH,\n",
    "    depth=DEPTH,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    act_fn=ACT_FN,\n",
    "    use_bias=True\n",
    ")\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train and test\n",
    "\n",
    "A PC network can be updated in a single line of code with [`jpc.make_pc_step()`](https://thebuckleylab.github.io/jpc/api/Training/#jpc.make_pc_step). Similarly, we can use [`jpc.test_discriminative_pc()`](https://thebuckleylab.github.io/jpc/api/Testing/#jpc.test_discriminative_pc) to compute the network accuracy. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_logs(writer, iteration, result):\n",
    "    writer.add_scalar(f'base/loss', float(result[\"loss\"]), iteration)\n",
    "    writer.add_scalar(f'base/t_max', float(result[\"t_max\"]), iteration)\n",
    "    if \"acc\" in result:\n",
    "        writer.add_scalar(f'base/acc', float(result[\"acc\"]), iteration)\n",
    "\n",
    "    # Log gradient norms for each layer\n",
    "    for layer_idx, param_grad in enumerate(result['model_param_grads']):\n",
    "        writer.add_scalar(f'grad/layer_{layer_idx}_weight_grad', float(jax.numpy.linalg.norm(param_grad[1].weight)), iteration)\n",
    "        writer.add_scalar(f'grad/layer_{layer_idx}_bias_grad', float(jax.numpy.linalg.norm(param_grad[1].bias)), iteration)\n",
    "\n",
    "    # Log energies for each layer - they will be grouped by custom scalars config\n",
    "    for layer_idx, energy in enumerate(result[\"energies\"]):\n",
    "        writer.add_scalar(f'energies/layer_{layer_idx}', float(energy), iteration)\n",
    "\n",
    "    # log activity for each layer\n",
    "    for layer_idx, activity in enumerate(result[\"activities\"]):\n",
    "        writer.add_scalar(f'activations_raw/layer_{layer_idx}', float(jax.numpy.linalg.norm(activity)), iteration)\n",
    "\n",
    "\n",
    "\n",
    "    # log activity norms for each layer\n",
    "    if result[\"activity_norms\"] is not None:\n",
    "        for layer_idx, activity in enumerate(result[\"activity_norms\"]):\n",
    "            writer.add_scalar(f'activations_norm/layer_{layer_idx}', float(jax.numpy.linalg.norm(activity)), iteration)\n",
    "\n",
    "\n",
    "    # log momentums for each layer from optimizer result[\"opt_state\"][0].mu\n",
    "    # mu[0]: model; mu[1]: None (skip model)\n",
    "    for layer_idx, momentum in enumerate(result['opt_state'][0].mu[0]):\n",
    "        writer.add_scalar(f'momentum/layer_{layer_idx}_weight_momentum', float(jax.numpy.linalg.norm(momentum[1].weight)), iteration)\n",
    "        writer.add_scalar(f'momentum/layer_{layer_idx}_bias_momentum', float(jax.numpy.linalg.norm(momentum[1].bias)), iteration)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    avg_test_loss, avg_test_acc = 0, 0\n",
    "    for _, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "            \n",
    "        test_loss, test_acc = jpc.test_discriminative_pc(\n",
    "            model=model,\n",
    "            input=img_batch,\n",
    "            output=label_batch\n",
    "        )\n",
    "        avg_test_loss += test_loss\n",
    "        avg_test_acc += test_acc\n",
    "\n",
    "    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "      model,\n",
    "      lr,\n",
    "      batch_size,\n",
    "      test_every,\n",
    "      n_train_iters,\n",
    "      writer = None,\n",
    "      log_every = 10,\n",
    "    #   save_gradients = True\n",
    "):\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(\n",
    "        (eqx.filter(model, eqx.is_array), None)\n",
    "    )\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "\n",
    "\n",
    "    CALCULATE_ACCURACY = True\n",
    "    ACTIVITY_NORMS = True\n",
    "    for iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        result = jpc.make_pc_step(\n",
    "            model=model,\n",
    "            optim=optim,\n",
    "            opt_state=opt_state,\n",
    "            output=label_batch,\n",
    "            input=img_batch,\n",
    "            calculate_accuracy = CALCULATE_ACCURACY,\n",
    "            activity_norms = ACTIVITY_NORMS\n",
    "        )\n",
    "\n",
    "        if writer is not None and (iter % log_every) == 0:\n",
    "            record_logs(writer, iter, result)\n",
    "\n",
    "        model, opt_state = result[\"model\"], result[\"opt_state\"]\n",
    "        train_loss = result[\"loss\"]\n",
    "        if ((iter+1) % test_every) == 0:\n",
    "            _, avg_test_acc = evaluate(model, test_loader)\n",
    "            print(\n",
    "                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (iter+1) >= n_train_iters:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 100, train loss=0.009165, avg test accuracy=93.409454\n",
      "Train iter 200, train loss=0.006444, avg test accuracy=95.402641\n",
      "Train iter 300, train loss=0.005436, avg test accuracy=95.753204\n"
     ]
    }
   ],
   "source": [
    "def create_writer(log_dir):\n",
    "    if type(log_dir) is str:\n",
    "        log_dir = Path(log_dir)\n",
    "\n",
    "    paths = sorted(list(log_dir.glob('*/')))\n",
    "    if len(paths) == 0:\n",
    "        return SummaryWriter(log_dir=log_dir / 'run_000')\n",
    "    last_index = int(paths[-1].name.split('_')[-1])\n",
    "    new_index = last_index + 1\n",
    "    return SummaryWriter(log_dir=log_dir / f'run_{new_index:03d}')\n",
    "\n",
    "\n",
    "writer = create_writer(log_dir='runs/my_experiment')\n",
    "\n",
    "train(\n",
    "    model=network,\n",
    "    lr=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_path = sorted((Path(writer.log_dir) / 'gradients').glob('*.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer_0_weight', 'layer_0_bias', 'layer_1_weight', 'layer_1_bias', 'layer_2_weight', 'layer_2_bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(grad_path[0], 'rb') as f:\n",
    "    gradient_data = pickle.load(f)\n",
    "\n",
    "gradient_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # for key, value in result.items():\n",
    "            #     if key not in [\"model\", \"skip_model\", \"opt_state\", \"activities\"] and value is not None:\n",
    "            #         writer.add_scalar(f'train/{key}', value, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Saved Gradients\n",
    "\n",
    "After training, you can load and visualize the saved gradient data offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "def plot_gradients_from_file(grad_file, max_params=200):\n",
    "    \"\"\"Load and plot gradients from a saved pickle file.\"\"\"\n",
    "    with open(grad_file, 'rb') as f:\n",
    "        gradient_data = pickle.load(f)\n",
    "    \n",
    "    # Extract iteration number from filename\n",
    "    iter_num = int(grad_file.split('_iter_')[-1].split('.pkl')[0])\n",
    "    \n",
    "    # Get number of layers\n",
    "    n_layers = len([k for k in gradient_data.keys() if 'weight' in k])\n",
    "    \n",
    "    fig, axes = plt.subplots(n_layers, 1, figsize=(12, 3*n_layers))\n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        weight_grad = gradient_data[f'layer_{layer_idx}_weight']\n",
    "        grad_flat = weight_grad.flatten()\n",
    "        \n",
    "        # Sample if too many parameters\n",
    "        if len(grad_flat) > max_params:\n",
    "            indices = np.linspace(0, len(grad_flat)-1, max_params, dtype=int)\n",
    "            grad_subset = grad_flat[indices]\n",
    "        else:\n",
    "            grad_subset = grad_flat\n",
    "        \n",
    "        # Plot\n",
    "        axes[layer_idx].bar(range(len(grad_subset)), grad_subset, width=1.0)\n",
    "        axes[layer_idx].set_xlabel('Parameter Index')\n",
    "        axes[layer_idx].set_ylabel('Gradient Value')\n",
    "        axes[layer_idx].set_title(f'Layer {layer_idx} Weight Gradients (Iteration {iter_num})')\n",
    "        axes[layer_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example: Plot gradients from a specific iteration\n",
    "# grad_dir = 'runs/my_experiment/gradients'\n",
    "# grad_files = sorted(glob.glob(f'{grad_dir}/gradients_iter_*.pkl'))\n",
    "# if grad_files:\n",
    "#     # Plot the first saved iteration\n",
    "#     fig = plot_gradients_from_file(grad_files[0])\n",
    "#     plt.show()\n",
    "#     \n",
    "#     # Or plot all iterations\n",
    "#     for grad_file in grad_files:\n",
    "#         fig = plot_gradients_from_file(grad_file)\n",
    "#         plt.savefig(grad_file.replace('.pkl', '.png'))\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
